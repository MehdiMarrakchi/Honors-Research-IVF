{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf4400ba-a085-43db-9089-be86fb09786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the libraries we need\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b93a870e-48e8-4e92-8d92-954f8006d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the folder where your CSVs live\n",
    "DATA_DIR = Path(\".\")  # Change this if your CSVs are in a different folder\n",
    "\n",
    "# QB-RAG-8 settings\n",
    "NUM_QUESTIONS_PER_CHUNK = 20   # Generate 8 questions per chunk (the \"8\" in QB-RAG-8)\n",
    "CHUNK_SIZE = 300              # Each chunk is roughly 300 words\n",
    "CHUNK_OVERLAP = 50            # Chunks overlap by 50 words so we don't cut mid-sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baae59d8-d638-455d-9d04-c7764b537b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Type of contact: Visit<br/><strong>Created by: </strong>mary.dumalag\n",
      "After:  Type of contact: Visit Created by: mary.dumalag\n"
     ]
    }
   ],
   "source": [
    "def strip_html(text):\n",
    "    \"\"\"\n",
    "    Your clinical data has HTML tags embedded in it like <br/>, <strong>, <p>, etc.\n",
    "    This function turns that into clean readable text.\n",
    "    \n",
    "    Example:\n",
    "      Input:  \"Type of contact: Visit<br/><strong>Created by: </strong>mary\"\n",
    "      Output: \"Type of contact: Visit Created by: mary\"\n",
    "    \"\"\"\n",
    "    if pd.isna(text):           # Handle missing/NaN values\n",
    "        return \"\"\n",
    "    text = unescape(str(text))  # Convert &amp; â†’ &, &#8217; â†’ ', etc.\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean = soup.get_text(separator=\" \", strip=True)  # Remove all HTML tags, keep text\n",
    "    return re.sub(r'\\s+', ' ', clean).strip()         # Collapse extra whitespace\n",
    "\n",
    "\n",
    "# Quick test to see it works\n",
    "raw = \"Type of contact: Visit<br/><strong>Created by: </strong>mary.dumalag\"\n",
    "print(f\"Before: {raw}\")\n",
    "print(f\"After:  {strip_html(raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8df06fc-1dbd-4753-831e-6b9ddaf5ce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycles: 17 rows, 22 columns\n",
      "reproduction_episodes: 1 rows, 34 columns\n",
      "follow_up_notes: 557 rows, 6 columns\n",
      "spermiogram: 1 rows, 16 columns\n",
      "history_visits: 56 rows, 7 columns\n"
     ]
    }
   ],
   "source": [
    "def load_and_clean_all(data_dir):\n",
    "    \"\"\"\n",
    "    Load all 5 CSVs, parse dates, and clean HTML from text fields.\n",
    "    Returns a dictionary where each key is a table name and each value is a DataFrame.\n",
    "    \"\"\"\n",
    "    # 1. Cycles\n",
    "    cycles = pd.read_csv(data_dir / \"Cycles.csv\", encoding=\"utf-8-sig\")\n",
    "    cycles[\"Date\"] = pd.to_datetime(cycles[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    \n",
    "    # 2. Reproduction Episodes â€” has HTML in several columns\n",
    "    repro = pd.read_csv(data_dir / \"Reproduction Episodes.csv\", encoding=\"utf-8-sig\")\n",
    "    repro[\"Date\"] = pd.to_datetime(repro[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    for col in [\"Therapeutic Plan\", \"Cause of the episode\", \"Obstetrics observations\"]:\n",
    "        if col in repro.columns:\n",
    "            repro[col] = repro[col].apply(strip_html)\n",
    "    \n",
    "    # 3. Follow-up Notes â€” has HTML in the main text column\n",
    "    notes = pd.read_csv(data_dir / \"Follow up notes.csv\", encoding=\"utf-8-sig\")\n",
    "    notes[\"Date\"] = pd.to_datetime(notes[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    notes[\"Follow-up_clean\"] = notes[\"Follow-up\"].apply(strip_html)\n",
    "    notes = notes.sort_values(\"Date\")\n",
    "    \n",
    "    # 4. Spermiogram\n",
    "    sperm = pd.read_csv(data_dir / \"Spermiogram.csv\", encoding=\"utf-8-sig\")\n",
    "    sperm[\"Date\"] = pd.to_datetime(sperm[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    sperm[\"Remarks_clean\"] = sperm[\"Remarks\"].apply(strip_html)\n",
    "    \n",
    "    # 5. History Visits (BMI tracking)\n",
    "    visits = pd.read_csv(data_dir / \"History visits.csv\", encoding=\"utf-8-sig\")\n",
    "    visits[\"Date\"] = pd.to_datetime(visits[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    visits = visits.sort_values(\"Date\")\n",
    "    \n",
    "    return {\n",
    "        \"cycles\": cycles,\n",
    "        \"reproduction_episodes\": repro,\n",
    "        \"follow_up_notes\": notes,\n",
    "        \"spermiogram\": sperm,\n",
    "        \"history_visits\": visits,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run it\n",
    "data = load_and_clean_all(DATA_DIR)\n",
    "\n",
    "# Check what we got\n",
    "for name, df in data.items():\n",
    "    print(f\"{name}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f7d8372-724f-4b9f-8345-ec6637f81d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  DUAL-PATH DATA PIPELINE\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š PATH 1: Loading structured data into SQLite...\n",
      "----------------------------------------\n",
      "  âœ“ cycles: 17 rows, 21 columns\n",
      "  âœ“ reproduction_episodes: 1 rows, 32 columns\n",
      "  âœ“ spermiogram: 1 rows, 16 columns\n",
      "  âœ“ history_visits: 56 rows, 7 columns\n",
      "\n",
      "Database saved to: clinic.db\n",
      "Tables created: ['cycles', 'reproduction_episodes', 'spermiogram', 'history_visits']\n",
      "\n",
      "ðŸ” Verifying database...\n",
      "----------------------------------------\n",
      "Database verification:\n",
      "==================================================\n",
      "\n",
      "  cycles: 17 rows, 21 columns\n",
      "  Columns: Id, Process, Protocol, Date, MD, Technique, Ooc__Source, Semen_source...\n",
      "\n",
      "  reproduction_episodes: 1 rows, 32 columns\n",
      "  Columns: Id, Date, Working_Centre, Patient_ID, Date_of_Birth, Gestations, Para, Misscarriage...\n",
      "\n",
      "  spermiogram: 1 rows, 16 columns\n",
      "  Columns: Id, Request_date, Date, External, Episode, Patient, Volume___1_4_ml, Count___16_M_ml...\n",
      "\n",
      "  history_visits: 56 rows, 7 columns\n",
      "  Columns: Code, Patient_ID, Date, Age, Height, Weight, BMI\n",
      "\n",
      "==================================================\n",
      "Sample queries:\n",
      "--------------------------------------------------\n",
      "\n",
      "  SELECT Date, Process, Technique, Outcome FROM cycles LIMIT 3:\n",
      "    ('2020-01-15 00:00:00', 'I.V.F Stimulation', 'ICSI', 'No transfer: Embryo vitrification')\n",
      "    ('2020-02-08 00:00:00', 'Endometrial Prep.', None, 'Pregnancy: Yes;')\n",
      "    ('2021-01-28 00:00:00', 'I.V.F Stimulation', 'VITRIFICATION (Own oocytes, Accumulation', 'No transfer: Oocyte vitrification')\n",
      "\n",
      "  SELECT Date, Age, BMI FROM history_visits ORDER BY Date LIMIT 5:\n",
      "    ('2021-06-14 00:00:00', 44, 32.27)\n",
      "    ('2021-06-20 00:00:00', 44, 32.27)\n",
      "    ('2021-06-26 00:00:00', 44, 31.29)\n",
      "    ('2021-08-15 00:00:00', 44, 31.25)\n",
      "    ('2021-08-21 00:00:00', 44, 31.25)\n",
      "\n",
      "  Cycles resulting in pregnancy: 2\n",
      "\n",
      "\n",
      "ðŸ“ PATH 2: Building clinical note chunks...\n",
      "----------------------------------------\n",
      "Total chunks: 669\n",
      "\n",
      "Chunks by source:\n",
      "  episode_cause: 1\n",
      "  episode_obstetrics: 1\n",
      "  follow_up_notes: 666\n",
      "  spermiogram_remarks: 1\n",
      "\n",
      "Average chunk size: 82 words (~109 tokens)\n",
      "\n",
      "Sample chunk (first 200 chars):\n",
      "  Source: follow_up_notes\n",
      "  Date: 2019-10-28 00:00:00\n",
      "  Text: Type of contact: Visit New consultation scan attached Created by: mary.dumalag...\n",
      "\n",
      "\n",
      "============================================================\n",
      "  PIPELINE COMPLETE\n",
      "============================================================\n",
      "  SQLite database : clinic.db\n",
      "  Structured tables: 4 tables ready for SQL queries\n",
      "  Text chunks      : 669 chunks ready for embedding\n",
      "  Schema descriptions: defined for all 4 tables\n",
      "============================================================\n",
      "Type of contact: Visit New consultation scan attached Created by: mary.dumalag\n",
      "{'source': 'follow_up_notes', 'date': '2019-10-28 00:00:00', 'type': 'Medical', 'note_id': '138119', 'patient_id': '1'}\n",
      "\n",
      "Type of contact: Notes PATIENT ID: 010177F934782 CHIEF COMPLAINT (CC): The patient is a 42-year-old \n",
      "{'source': 'follow_up_notes', 'date': '2019-10-28 00:00:00', 'type': 'Medical', 'note_id': '138320', 'patient_id': '1', 'chunk_index': 0}\n",
      "\n",
      "Positive family history of hypertension and diabetes mellitus. SOCIAL HISTORY: Married since 1 year.\n",
      "{'source': 'follow_up_notes', 'date': '2019-10-28 00:00:00', 'type': 'Medical', 'note_id': '138320', 'patient_id': '1', 'chunk_index': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Patient Data Pipeline â€” Dual Path Setup\n",
    "========================================\n",
    "Path 1: Structured CSVs â†’ SQLite (for SQL queries)\n",
    "Path 2: Clinical notes  â†’ Text chunks (for semantic search)\n",
    "\n",
    "Run this in your Jupyter notebook after loading your CSVs into the `data` dict.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PATH 1: STRUCTURED DATA â†’ SQLite\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def create_sqlite_database(data, db_path=\"clinic.db\"):\n",
    "    \"\"\"\n",
    "    Load structured CSV dataframes into a SQLite database.\n",
    "    Each dataframe becomes a SQL table that can be queried directly.\n",
    "    \n",
    "    Args:\n",
    "        data: dict of dataframes (your existing `data` variable)\n",
    "        db_path: where to save the database file\n",
    "    \n",
    "    Returns:\n",
    "        sqlite3 connection object\n",
    "    \"\"\"\n",
    "    # Remove old database if it exists so we start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # â”€â”€ Define which tables are STRUCTURED (go into SQL) â”€â”€\n",
    "    # These are the tables where doctors will ask numerical/filtering questions\n",
    "    structured_tables = {\n",
    "        \"cycles\": {\n",
    "            \"df_key\": \"cycles\",\n",
    "            \"table_name\": \"cycles\",\n",
    "        },\n",
    "        \"reproduction_episodes\": {\n",
    "            \"df_key\": \"reproduction_episodes\",\n",
    "            \"table_name\": \"reproduction_episodes\",\n",
    "        },\n",
    "        \"spermiogram\": {\n",
    "            \"df_key\": \"spermiogram\",\n",
    "            \"table_name\": \"spermiogram\",\n",
    "        },\n",
    "        \"history_visits\": {\n",
    "            \"df_key\": \"history_visits\",\n",
    "            \"table_name\": \"history_visits\",\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    tables_created = []\n",
    "    \n",
    "    for key, config in structured_tables.items():\n",
    "        df = data[config[\"df_key\"]].copy()\n",
    "        table_name = config[\"table_name\"]\n",
    "        \n",
    "        # Clean column names: remove special chars that break SQL\n",
    "        # e.g., \"#COC\" â†’ \"COC\", \"Specific Outcome \" â†’ \"Specific_Outcome\"\n",
    "        df.columns = [\n",
    "            re.sub(r'[^a-zA-Z0-9_]', '_', col.strip()).strip('_')\n",
    "            for col in df.columns\n",
    "        ]\n",
    "        \n",
    "        # Remove fully empty columns\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        \n",
    "        # Load into SQLite\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        tables_created.append(table_name)\n",
    "        \n",
    "        print(f\"  âœ“ {table_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    print(f\"\\nDatabase saved to: {db_path}\")\n",
    "    print(f\"Tables created: {tables_created}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "\n",
    "def get_schema_description():\n",
    "    \"\"\"\n",
    "    Return human-readable column descriptions for each table.\n",
    "    \n",
    "    This is CRITICAL for the Text-to-SQL engine â€” without it, the LLM\n",
    "    won't know that 'COC' means oocytes or that 'MII' means mature eggs.\n",
    "    The LLM reads these descriptions to write correct SQL queries.\n",
    "    \n",
    "    Update this as Raquel sends you new data with additional columns.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"cycles\": {\n",
    "            \"table_description\": \"IVF treatment cycles with outcomes and embryology results\",\n",
    "            \"columns\": {\n",
    "                \"Id\": \"Unique cycle identifier\",\n",
    "                \"Process\": \"Type of procedure (e.g., I.V.F Stimulation, Endometrial Prep.)\",\n",
    "                \"Protocol\": \"Stimulation protocol used (e.g., Antagonist fixed D5, Luteal phase stimulation)\",\n",
    "                \"Date\": \"Date the cycle started (DD/MM/YYYY)\",\n",
    "                \"MD\": \"Treating physician initials\",\n",
    "                \"Technique\": \"Lab technique (e.g., ICSI, VITRIFICATION)\",\n",
    "                \"Ooc__Source\": \"Source of oocytes (Fresh Own, Combined, etc.)\",\n",
    "                \"Semen_source\": \"Source of sperm (Fresh ejaculation, Frozen ejaculation, etc.)\",\n",
    "                \"Outcome\": \"Cycle result (Pregnancy: Yes, No transfer: Embryo vitrification, etc.)\",\n",
    "                \"Specific_Outcome\": \"Detailed outcome information\",\n",
    "                \"Live_birth\": \"Whether a live birth resulted (e.g., '1, NVD' = 1 baby, normal vaginal delivery)\",\n",
    "                \"PGT\": \"Pre-implantation genetic testing type (PGT-A = aneuploidy screening)\",\n",
    "                \"Ref__No_\": \"Reference number for the cycle\",\n",
    "                \"_ET\": \"Number of embryos transferred\",\n",
    "                \"_COC\": \"Number of cumulus-oocyte complexes (oocytes) retrieved\",\n",
    "                \"MII\": \"Number of metaphase II (mature) oocytes\",\n",
    "                \"_2PN\": \"Number of normally fertilized oocytes (2 pronuclei)\",\n",
    "                \"Frozen_oocytes\": \"Number of oocytes frozen (vitrified)\",\n",
    "                \"Frozen_embryo\": \"Number of embryos frozen\",\n",
    "                \"Transferable_Emb_\": \"Number of embryos suitable for transfer\",\n",
    "                \"Age\": \"Patient age at time of cycle\",\n",
    "                \"BMI\": \"Patient BMI at time of cycle\",\n",
    "            }\n",
    "        },\n",
    "        \"reproduction_episodes\": {\n",
    "            \"table_description\": \"Patient reproductive history, demographics, and episode information\",\n",
    "            \"columns\": {\n",
    "                \"Id\": \"Episode identifier\",\n",
    "                \"Date\": \"Episode start date\",\n",
    "                \"Working_Centre\": \"Clinic name\",\n",
    "                \"Patient_ID\": \"Anonymized patient identifier\",\n",
    "                \"Gestations\": \"Number of previous pregnancies\",\n",
    "                \"Para\": \"Number of births\",\n",
    "                \"Misscarriage\": \"Number of miscarriages\",\n",
    "                \"E_P\": \"Number of ectopic pregnancies\",\n",
    "                \"C_Sections\": \"Number of cesarean sections\",\n",
    "                \"Preterm_Birth\": \"Number of preterm births\",\n",
    "                \"Insurance_company\": \"Patient insurance provider\",\n",
    "                \"Partner_ID\": \"Anonymized partner identifier\",\n",
    "                \"Reason_Consultation\": \"Why the patient consulted (e.g., Primary infertility)\",\n",
    "                \"Therapeutic_Plan\": \"Planned treatment approach\",\n",
    "                \"Years_of_marriage\": \"Duration of marriage in years\",\n",
    "                \"Nationality\": \"Patient nationality\",\n",
    "                \"Gender\": \"Patient gender\",\n",
    "                \"Marital_status\": \"Current marital status\",\n",
    "                \"Cryopreserved_available\": \"Number of cryopreserved samples available\",\n",
    "                \"Total_cryopreserved\": \"Total cryopreserved samples\",\n",
    "                \"Cycle_length\": \"Menstrual cycle length in days\",\n",
    "                \"Previous_treatments\": \"Number of previous fertility treatments\",\n",
    "                \"Cause_of_the_episode\": \"Clinical cause/diagnosis for this episode\",\n",
    "            }\n",
    "        },\n",
    "        \"spermiogram\": {\n",
    "            \"table_description\": \"Semen analysis results for the male partner\",\n",
    "            \"columns\": {\n",
    "                \"Id\": \"Spermiogram test identifier\",\n",
    "                \"Request_date\": \"Date the test was requested\",\n",
    "                \"Date\": \"Date the test was performed\",\n",
    "                \"External\": \"Whether the test was done externally\",\n",
    "                \"Episode\": \"Related episode reference\",\n",
    "                \"Patient\": \"Patient (male partner) identifier\",\n",
    "                \"Volume____1_4_ml_\": \"Semen volume in ml (normal â‰¥ 1.4 ml)\",\n",
    "                \"Count____16_M_ml_\": \"Sperm count in millions per ml (normal â‰¥ 16 M/ml)\",\n",
    "                \"Progressive____30__\": \"Progressive motility percentage (normal â‰¥ 30%)\",\n",
    "                \"Normal_Forms____4__\": \"Normal morphology percentage (normal â‰¥ 4%)\",\n",
    "                \"Sperm_Motility_Recovery\": \"Total motile sperm after processing\",\n",
    "                \"Sperm_Motility_Recovery_ml\": \"Motile sperm per ml after processing\",\n",
    "                \"Diagnosis\": \"Spermiogram diagnosis (e.g., Normozoospermia, Oligozoospermia)\",\n",
    "                \"Partner\": \"Female partner identifier\",\n",
    "                \"Remarks\": \"Additional notes about the sample\",\n",
    "            }\n",
    "        },\n",
    "        \"history_visits\": {\n",
    "            \"table_description\": \"Patient visit history with anthropometric measurements over time\",\n",
    "            \"columns\": {\n",
    "                \"Code\": \"Visit identifier\",\n",
    "                \"Patient_ID\": \"Anonymized patient identifier\",\n",
    "                \"Date\": \"Visit date (DD/MM/YYYY)\",\n",
    "                \"Age\": \"Patient age at visit\",\n",
    "                \"Height\": \"Patient height in meters\",\n",
    "                \"Weight\": \"Patient weight in kg\",\n",
    "                \"BMI\": \"Body Mass Index (kg/mÂ²)\",\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def verify_database(conn):\n",
    "    \"\"\"\n",
    "    Quick sanity check â€” print each table's row count and a sample query.\n",
    "    Run this after creating the database to make sure everything loaded correctly.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # List all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    print(\"Database verification:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for table in tables:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = [row[1] for row in cursor.fetchall()]\n",
    "        \n",
    "        print(f\"\\n  {table}: {count} rows, {len(columns)} columns\")\n",
    "        print(f\"  Columns: {', '.join(columns[:8])}{'...' if len(columns) > 8 else ''}\")\n",
    "    \n",
    "    # Run a sample query to prove it works\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Sample queries:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"SELECT Date, Process, Technique, Outcome FROM cycles LIMIT 3\")\n",
    "        rows = cursor.fetchall()\n",
    "        print(\"\\n  SELECT Date, Process, Technique, Outcome FROM cycles LIMIT 3:\")\n",
    "        for row in rows:\n",
    "            print(f\"    {row}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Query failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"SELECT Date, Age, BMI FROM history_visits ORDER BY Date LIMIT 5\")\n",
    "        rows = cursor.fetchall()\n",
    "        print(\"\\n  SELECT Date, Age, BMI FROM history_visits ORDER BY Date LIMIT 5:\")\n",
    "        for row in rows:\n",
    "            print(f\"    {row}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Query failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM cycles WHERE Outcome LIKE '%Pregnancy%'\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"\\n  Cycles resulting in pregnancy: {count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Query failed: {e}\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PATH 2: CLINICAL NOTES â†’ TEXT CHUNKS (for semantic search)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def clean_html(text):\n",
    "    \"\"\"Strip HTML tags and clean up whitespace from clinical notes.\"\"\"\n",
    "    if pd.isna(text) or str(text).strip() == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)           # Remove HTML tags\n",
    "    text = re.sub(r'&[a-zA-Z]+;', ' ', text)       # Remove HTML entities like &amp;\n",
    "    text = re.sub(r'&#\\d+;', ' ', text)             # Remove numeric HTML entities like &#160;\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()        # Collapse whitespace\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_clinical_note_chunks(patient_id, data, max_chunk_tokens=300):\n",
    "    \"\"\"\n",
    "    Extract ONLY the unstructured clinical text and split into chunks\n",
    "    for vector search. This does NOT touch the structured data â€” that\n",
    "    lives in SQLite now.\n",
    "    \n",
    "    Args:\n",
    "        patient_id: which patient to extract notes for\n",
    "        data: dict of dataframes\n",
    "        max_chunk_tokens: approximate max tokens per chunk (1 token â‰ˆ 0.75 words)\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts, each with 'text', 'metadata' (date, type, source)\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    max_words = int(max_chunk_tokens * 0.75)  # Rough token-to-word conversion\n",
    "    \n",
    "    # â”€â”€ Source 1: Follow-up Notes â”€â”€\n",
    "    # These are the richest source of unstructured clinical text\n",
    "    notes_df = data[\"follow_up_notes\"]\n",
    "    if \"Patient ID\" in notes_df.columns:\n",
    "        notes_df = notes_df[notes_df[\"Patient ID\"] == patient_id]\n",
    "    \n",
    "    for _, row in notes_df.iterrows():\n",
    "        # Use cleaned version if available, otherwise clean on the fly\n",
    "        if \"Follow-up_clean\" in notes_df.columns:\n",
    "            text = str(row.get(\"Follow-up_clean\", \"\"))\n",
    "        else:\n",
    "            text = clean_html(row.get(\"Follow-up\", \"\"))\n",
    "        \n",
    "        if not text or len(text.strip()) < 20:\n",
    "            continue\n",
    "        \n",
    "        metadata = {\n",
    "            \"source\": \"follow_up_notes\",\n",
    "            \"date\": str(row.get(\"Date\", \"\")),\n",
    "            \"type\": str(row.get(\"Type\", \"\")),\n",
    "            \"note_id\": str(row.get(\"Id\", \"\")),\n",
    "            \"patient_id\": str(patient_id),\n",
    "        }\n",
    "        \n",
    "        # Split long notes into sub-chunks with overlap\n",
    "        words = text.split()\n",
    "        if len(words) <= max_words:\n",
    "            chunks.append({\"text\": text, \"metadata\": metadata})\n",
    "        else:\n",
    "            # Sliding window with ~50 token (37 word) overlap\n",
    "            overlap_words = 37\n",
    "            start = 0\n",
    "            chunk_idx = 0\n",
    "            while start < len(words):\n",
    "                end = start + max_words\n",
    "                chunk_text = \" \".join(words[start:end])\n",
    "                chunk_meta = {**metadata, \"chunk_index\": chunk_idx}\n",
    "                chunks.append({\"text\": chunk_text, \"metadata\": chunk_meta})\n",
    "                start = end - overlap_words\n",
    "                chunk_idx += 1\n",
    "    \n",
    "    # â”€â”€ Source 2: Free-text fields from Reproduction Episodes â”€â”€\n",
    "    # Cause of the episode and Therapeutic Plan often contain narrative text\n",
    "    episodes_df = data[\"reproduction_episodes\"]\n",
    "    if \"Patient ID\" in episodes_df.columns:\n",
    "        episodes_df = episodes_df[episodes_df[\"Patient ID\"] == patient_id]\n",
    "    \n",
    "    freetext_columns = [\n",
    "        (\"Cause of the episode\", \"episode_cause\"),\n",
    "        (\"Therapeutic Plan\", \"episode_therapeutic_plan\"),\n",
    "        (\"Obstetrics observations\", \"episode_obstetrics\"),\n",
    "    ]\n",
    "    \n",
    "    for col_name, source_label in freetext_columns:\n",
    "        if col_name not in episodes_df.columns:\n",
    "            continue\n",
    "        for _, row in episodes_df.iterrows():\n",
    "            text = clean_html(row.get(col_name, \"\"))\n",
    "            if not text or len(text.strip()) < 20:\n",
    "                continue\n",
    "            \n",
    "            metadata = {\n",
    "                \"source\": source_label,\n",
    "                \"date\": str(row.get(\"Date\", \"\")),\n",
    "                \"patient_id\": str(patient_id),\n",
    "            }\n",
    "            \n",
    "            words = text.split()\n",
    "            if len(words) <= max_words:\n",
    "                chunks.append({\"text\": text, \"metadata\": metadata})\n",
    "            else:\n",
    "                overlap_words = 37\n",
    "                start = 0\n",
    "                chunk_idx = 0\n",
    "                while start < len(words):\n",
    "                    end = start + max_words\n",
    "                    chunk_text = \" \".join(words[start:end])\n",
    "                    chunk_meta = {**metadata, \"chunk_index\": chunk_idx}\n",
    "                    chunks.append({\"text\": chunk_text, \"metadata\": chunk_meta})\n",
    "                    start = end - overlap_words\n",
    "                    chunk_idx += 1\n",
    "    \n",
    "    # â”€â”€ Source 3: Spermiogram Remarks â”€â”€\n",
    "    sperm_df = data[\"spermiogram\"]\n",
    "    for _, row in sperm_df.iterrows():\n",
    "        text = clean_html(row.get(\"Remarks\", \"\"))\n",
    "        if not text or len(text.strip()) < 20:\n",
    "            continue\n",
    "        chunks.append({\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"source\": \"spermiogram_remarks\",\n",
    "                \"date\": str(row.get(\"Date\", \"\")),\n",
    "                \"patient_id\": str(patient_id),\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def print_chunk_summary(chunks):\n",
    "    \"\"\"Print a summary of the generated chunks.\"\"\"\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    print()\n",
    "    \n",
    "    # Count by source\n",
    "    sources = {}\n",
    "    for chunk in chunks:\n",
    "        src = chunk[\"metadata\"][\"source\"]\n",
    "        sources[src] = sources.get(src, 0) + 1\n",
    "    \n",
    "    print(\"Chunks by source:\")\n",
    "    for src, count in sorted(sources.items()):\n",
    "        print(f\"  {src}: {count}\")\n",
    "    \n",
    "    # Show average chunk size\n",
    "    avg_words = sum(len(c[\"text\"].split()) for c in chunks) / len(chunks) if chunks else 0\n",
    "    print(f\"\\nAverage chunk size: {avg_words:.0f} words (~{avg_words / 0.75:.0f} tokens)\")\n",
    "    \n",
    "    # Preview first chunk\n",
    "    if chunks:\n",
    "        print(f\"\\nSample chunk (first 200 chars):\")\n",
    "        print(f\"  Source: {chunks[0]['metadata']['source']}\")\n",
    "        print(f\"  Date: {chunks[0]['metadata']['date']}\")\n",
    "        print(f\"  Text: {chunks[0]['text'][:200]}...\")\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MAIN: Run both paths\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def build_dual_path_pipeline(data, patient_id=1, db_path=\"clinic.db\"):\n",
    "    \"\"\"\n",
    "    Run the complete data ingestion pipeline:\n",
    "      - Path 1: Structured tables â†’ SQLite\n",
    "      - Path 2: Clinical notes â†’ text chunks (ready for embedding)\n",
    "    \n",
    "    Args:\n",
    "        data: dict of dataframes (your existing loaded CSVs)\n",
    "        patient_id: which patient to process\n",
    "        db_path: where to save the SQLite database\n",
    "    \n",
    "    Returns:\n",
    "        conn: SQLite connection (for structured queries)\n",
    "        chunks: list of text chunks with metadata (for vector search)\n",
    "        schema: column descriptions (for the LLM to understand the tables)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  DUAL-PATH DATA PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # â”€â”€ Path 1: Structured â†’ SQLite â”€â”€\n",
    "    print(\"\\nðŸ“Š PATH 1: Loading structured data into SQLite...\")\n",
    "    print(\"-\" * 40)\n",
    "    conn = create_sqlite_database(data, db_path)\n",
    "    \n",
    "    print(\"\\nðŸ” Verifying database...\")\n",
    "    print(\"-\" * 40)\n",
    "    verify_database(conn)\n",
    "    \n",
    "    # â”€â”€ Path 2: Notes â†’ Chunks â”€â”€\n",
    "    print(\"\\n\\nðŸ“ PATH 2: Building clinical note chunks...\")\n",
    "    print(\"-\" * 40)\n",
    "    chunks = build_clinical_note_chunks(patient_id, data)\n",
    "    print_chunk_summary(chunks)\n",
    "    \n",
    "    # â”€â”€ Schema descriptions â”€â”€\n",
    "    schema = get_schema_description()\n",
    "    \n",
    "    # â”€â”€ Summary â”€â”€\n",
    "    print(\"\\n\\n\" + \"=\" * 60)\n",
    "    print(\"  PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  SQLite database : {db_path}\")\n",
    "    print(f\"  Structured tables: {len(schema)} tables ready for SQL queries\")\n",
    "    print(f\"  Text chunks      : {len(chunks)} chunks ready for embedding\")\n",
    "    print(f\"  Schema descriptions: defined for all {len(schema)} tables\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return conn, chunks, schema\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# USAGE (paste this into your notebook cell):\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "conn, chunks, schema = build_dual_path_pipeline(data, patient_id=1)\n",
    "#\n",
    "#   # Now you can query SQL directly:\n",
    "pd.read_sql(\"SELECT Date, Outcome FROM cycles\", conn)\n",
    "#\n",
    "#   # And chunks are ready to embed into ChromaDB:\n",
    "for chunk in chunks[:3]:\n",
    "    print(chunk[\"text\"][:100])\n",
    "    print(chunk[\"metadata\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "527da47b-f5b5-4525-b817-c50d244951e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 244\n",
      "Words per chunk: ~300\n",
      "Overlap: 50 words\n",
      "\n",
      "--- Chunk 0 ---\n",
      "=== PATIENT ANAMNESIS === Id: 1070 | Date: 2019-10-28 00:00:00 | Working Centre: ART Fertility Clinics | Patient ID: 1 | *Date of Birth: 28126 | Gestations: 2 | Para: 3 | Misscarriage: 0 | E.P: 0 | C Sections: 1 | Preterm Birth: 1 | Insurance company: DAMAN THIQA | Partner ID: 2 | *Date of Birth Par\n",
      "\n",
      "--- Chunk 1 ---\n",
      "CYCLES === Id: 7571 | Process: I.V.F Stimulation | Protocol: Antagonist fixed D5 | Date: 2020-01-15 00:00:00 | MD: HF | Technique: ICSI | Semen source: Frozen ejaculation | Outcome: No transfer: Embryo vitrification | PGT: PGT-A (Screening of aneuploidies) | Ref. No.: C4700 | #ET: 0 | #COC: 19.0 | M\n"
     ]
    }
   ],
   "source": [
    "def chunk_document(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Split the patient document into overlapping pieces.\n",
    "    \n",
    "    Why? The full document is ~52,000 words â€” way too big to search \n",
    "    efficiently or fit in an LLM context. So we break it into ~300-word \n",
    "    pieces. The 50-word overlap ensures we don't lose information at \n",
    "    the boundaries between chunks.\n",
    "    \n",
    "    This is equivalent to the \"content cards\" in the QB-RAG paper.\n",
    "    Each chunk will later get 8 questions generated for it.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    chunk_id = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_text = \" \".join(words[start:end])\n",
    "        \n",
    "        chunks.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text,\n",
    "        })\n",
    "        \n",
    "        chunk_id += 1\n",
    "        start += chunk_size - overlap  # Step forward, but keep overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Run it\n",
    "chunks = chunk_document(patient_doc)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Words per chunk: ~{CHUNK_SIZE}\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} words\")\n",
    "\n",
    "# Look at the first 2 chunks\n",
    "print(f\"\\n--- Chunk 0 ---\")\n",
    "print(chunks[0][\"text\"][:300])\n",
    "print(f\"\\n--- Chunk 1 ---\")\n",
    "print(chunks[1][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8c8006e-0ef8-487c-a5da-a7da99d6bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template loaded.\n",
      "Each chunk will get 20 questions generated.\n"
     ]
    }
   ],
   "source": [
    "# This is the prompt we send to an LLM for each chunk.\n",
    "# Adapted from QB-RAG paper Appendix H.1, but for fertility clinic data\n",
    "# instead of diabetes content cards.\n",
    "\n",
    "QUESTION_GENERATION_PROMPT = \"\"\"You are a fertility clinic physician. Your task is to generate {num_questions} questions that a clinician might ask, which can be answered using ONLY the provided clinical text.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Generate exactly {num_questions} questions\n",
    "- Questions should be diverse: cover different facts in the text\n",
    "- Questions should be answerable from this text alone\n",
    "- Questions should be in the style a doctor or nurse would ask about a patient\n",
    "- Include questions about: diagnoses, lab values, treatment plans, medications, \n",
    "  outcomes, dates, measurements, and patient history where relevant\n",
    "- Do NOT copy the text verbatim into the questions\n",
    "\n",
    "EXAMPLES of good clinical questions:\n",
    "- \"What was the patient's BMI at their most recent visit?\"\n",
    "- \"What was the sperm diagnosis from the latest spermiogram?\"\n",
    "- \"Did the patient have any successful embryo transfers?\"\n",
    "- \"What medications were prescribed during the last consultation?\"\n",
    "\n",
    "Text:\n",
    "{chunk_text}\n",
    "\n",
    "Generate exactly {num_questions} questions as a JSON list:\n",
    "{{\"questions\": [\"question1\", \"question2\", ...]}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt template loaded.\")\n",
    "print(f\"Each chunk will get {NUM_QUESTIONS_PER_CHUNK} questions generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce9a1cc9-3477-4ce2-90d0-2bf32b443644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Error on chunk 0: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CYNr8q9WH1MkmdaFnHyBL'}\n",
      "Generated 0 questions for chunk 0:\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50670088-be16-4871-ab59-1066cd4a2480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
