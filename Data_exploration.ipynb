{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0-overview",
   "metadata": {},
   "source": [
    "# Phase 1: Data Ingestion Pipeline\n",
    "\n",
    "Preprocesses raw IVF clinical data into two data stores for a RAG system:\n",
    "\n",
    "| Step | What | Output |\n",
    "|------|------|--------|\n",
    "| 1 | **Anonymize** identifiers (SHA-256 hash) | Anonymized DataFrames + mapping file |\n",
    "| 2 | **Load structured data** into SQLite | `clinic.db` with 4 tables + schema descriptions |\n",
    "| 3 | **Process clinical notes** — strip HTML, build single document, chunk | Text chunks ready for embedding |\n",
    "| 4 | **Embed and store** in ChromaDB | Vector index (setup only — run when ready) |\n",
    "\n",
    "> **Note:** This notebook only runs preprocessing (Steps 1-3). Step 4 embedding code is provided but not auto-executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ──\n",
    "DATA_DIR = Path(\".\")  # Folder containing the 5 CSVs\n",
    "\n",
    "# Chunking settings\n",
    "CHUNK_SIZE = 300    # ~300 tokens per chunk\n",
    "CHUNK_OVERLAP = 50  # 50-token overlap between consecutive chunks\n",
    "\n",
    "# Identifier columns to hash (adapted from anonymize_data.py)\n",
    "IDENTIFIER_COLUMNS = [\n",
    "    \"Patient ID\",   # Follow-up notes, Reproduction Episodes, History Visits\n",
    "    \"Partner ID\",   # Reproduction Episodes\n",
    "    \"Patient\",      # Spermiogram\n",
    "    \"Partner\",      # Spermiogram\n",
    "]\n",
    "\n",
    "# SQLite output\n",
    "DB_PATH = \"clinic.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0-step1-header",
   "metadata": {},
   "source": [
    "## Step 1 — Anonymize Identifiers\n",
    "\n",
    "Adapted from `anonymize_data.py` which hashes fields in JSON/JSONL files using SHA-256.\n",
    "Here we apply the same hashing to identifier columns in pandas DataFrames loaded from CSV.\n",
    "The mapping is saved to `anonymization_mapping.json` for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1-anon-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value(value):\n",
    "    \"\"\"\n",
    "    Hash a value using SHA-256.\n",
    "    Identical logic to anonymize_data.py hash_value(), adapted for DataFrames.\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value is None:\n",
    "        return value\n",
    "    return hashlib.sha256(str(value).encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def anonymize_dataframes(data_dict, id_columns):\n",
    "    \"\"\"\n",
    "    Apply SHA-256 hashing to identifier columns across all DataFrames.\n",
    "\n",
    "    Adapted from anonymize_data.py (which processes JSON/JSONL files with\n",
    "    anonymize_dict()) to work on pandas DataFrames from CSV.\n",
    "\n",
    "    Args:\n",
    "        data_dict: dict of {table_name: DataFrame}\n",
    "        id_columns: list of column names to hash\n",
    "\n",
    "    Returns:\n",
    "        data_dict with identifier columns hashed in-place\n",
    "        mapping: {hashed_value: original_value} for traceability\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    for table_name, df in data_dict.items():\n",
    "        for col in id_columns:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            # Build mapping before overwriting\n",
    "            for val in df[col].dropna().unique():\n",
    "                hashed = hash_value(val)\n",
    "                if hashed not in mapping:\n",
    "                    mapping[hashed] = str(val)\n",
    "            # Replace with hashes\n",
    "            df[col] = df[col].apply(hash_value)\n",
    "            print(f\"  Hashed '{col}' in {table_name} ({df[col].notna().sum()} values)\")\n",
    "\n",
    "    return data_dict, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2-strip-html",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    \"\"\"\n",
    "    Strip HTML tags from clinical notes and clean up whitespace.\n",
    "    Clinical data has embedded tags like <br/>, <strong>, <p>, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = unescape(str(text))  # &amp; -> &, &#8217; -> ', etc.\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean = soup.get_text(separator=\" \", strip=True)\n",
    "    return re.sub(r'\\s+', ' ', clean).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3-load-tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tables(data_dir):\n",
    "    \"\"\"Load all 5 CSVs and parse date columns.\"\"\"\n",
    "    cycles = pd.read_csv(data_dir / \"Cycles.csv\", encoding=\"utf-8-sig\")\n",
    "    cycles[\"Date\"] = pd.to_datetime(cycles[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "    repro = pd.read_csv(data_dir / \"Reproduction Episodes.csv\", encoding=\"utf-8-sig\")\n",
    "    repro[\"Date\"] = pd.to_datetime(repro[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "    notes = pd.read_csv(data_dir / \"Follow up notes.csv\", encoding=\"utf-8-sig\")\n",
    "    notes[\"Date\"] = pd.to_datetime(notes[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    notes = notes.sort_values(\"Date\")\n",
    "\n",
    "    sperm = pd.read_csv(data_dir / \"Spermiogram.csv\", encoding=\"utf-8-sig\")\n",
    "    sperm[\"Date\"] = pd.to_datetime(sperm[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "    visits = pd.read_csv(data_dir / \"History visits.csv\", encoding=\"utf-8-sig\")\n",
    "    visits[\"Date\"] = pd.to_datetime(visits[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    visits = visits.sort_values(\"Date\")\n",
    "\n",
    "    return {\n",
    "        \"cycles\": cycles,\n",
    "        \"reproduction_episodes\": repro,\n",
    "        \"follow_up_notes\": notes,\n",
    "        \"spermiogram\": sperm,\n",
    "        \"history_visits\": visits,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load tables\n",
    "data = load_all_tables(DATA_DIR)\n",
    "for name, df in data.items():\n",
    "    print(f\"{name}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4-anonymize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anonymize identifier columns\n",
    "print(\"Anonymizing identifiers...\")\n",
    "data, id_mapping = anonymize_dataframes(data, IDENTIFIER_COLUMNS)\n",
    "\n",
    "# Save mapping for traceability (same structure as anonymize_data.py output)\n",
    "mapping_output = {\n",
    "    \"hash_to_original\": id_mapping,\n",
    "    \"original_to_hash\": {v: k for k, v in id_mapping.items()},\n",
    "    \"total_mappings\": len(id_mapping),\n",
    "}\n",
    "with open(\"anonymization_mapping.json\", \"w\") as f:\n",
    "    json.dump(mapping_output, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved {len(id_mapping)} identifier mapping(s) to anonymization_mapping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5-clean-html",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip HTML from text-heavy columns\n",
    "for col in [\"Therapeutic Plan\", \"Cause of the episode\", \"Obstetrics observations\"]:\n",
    "    if col in data[\"reproduction_episodes\"].columns:\n",
    "        data[\"reproduction_episodes\"][col] = data[\"reproduction_episodes\"][col].apply(strip_html)\n",
    "\n",
    "data[\"follow_up_notes\"][\"Follow-up_clean\"] = data[\"follow_up_notes\"][\"Follow-up\"].apply(strip_html)\n",
    "data[\"spermiogram\"][\"Remarks_clean\"] = data[\"spermiogram\"][\"Remarks\"].apply(strip_html)\n",
    "\n",
    "print(\"HTML stripped from clinical text columns.\")\n",
    "print(f\"  Follow-up notes: {data['follow_up_notes']['Follow-up_clean'].str.len().mean():.0f} avg chars per note\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0-step2-header",
   "metadata": {},
   "source": [
    "## Step 2 — Load Structured Data into SQLite\n",
    "\n",
    "Takes the anonymized Cycles, History Visits, Spermiogram, and Reproduction Episodes\n",
    "and loads them as SQL tables using `pandas.to_sql()`. A schema description dict tells\n",
    "the LLM what each column means (e.g., `#COC` = oocytes retrieved, `MII` = mature oocytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1-sqlite-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sqlite_database(data, db_path):\n",
    "    \"\"\"Load structured DataFrames into a SQLite database.\"\"\"\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    structured_tables = [\"cycles\", \"reproduction_episodes\", \"spermiogram\", \"history_visits\"]\n",
    "\n",
    "    for table_name in structured_tables:\n",
    "        df = data[table_name].copy()\n",
    "        # Clean column names for SQL compatibility\n",
    "        df.columns = [\n",
    "            re.sub(r'[^a-zA-Z0-9_]', '_', col.strip()).strip('_')\n",
    "            for col in df.columns\n",
    "        ]\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(f\"  {table_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "\n",
    "    print(f\"\\nDatabase saved to: {db_path}\")\n",
    "    return conn\n",
    "\n",
    "\n",
    "# Schema descriptions — critical for the Text-to-SQL engine\n",
    "# The LLM reads these to understand domain-specific column names\n",
    "SCHEMA_DESCRIPTION = {\n",
    "    \"cycles\": {\n",
    "        \"table_description\": \"IVF treatment cycles with outcomes and embryology results\",\n",
    "        \"columns\": {\n",
    "            \"Id\": \"Unique cycle identifier\",\n",
    "            \"Process\": \"Type of procedure (e.g., I.V.F Stimulation, Endometrial Prep.)\",\n",
    "            \"Protocol\": \"Stimulation protocol used (e.g., Antagonist fixed D5)\",\n",
    "            \"Date\": \"Date the cycle started\",\n",
    "            \"MD\": \"Treating physician initials\",\n",
    "            \"Technique\": \"Lab technique (e.g., ICSI, VITRIFICATION)\",\n",
    "            \"Ooc__Source\": \"Source of oocytes (Fresh Own, Combined, etc.)\",\n",
    "            \"Semen_source\": \"Source of sperm (Fresh ejaculation, Frozen ejaculation, etc.)\",\n",
    "            \"Outcome\": \"Cycle result (Pregnancy: Yes, No transfer: Embryo vitrification, etc.)\",\n",
    "            \"Specific_Outcome\": \"Detailed outcome information\",\n",
    "            \"Live_birth\": \"Whether a live birth resulted\",\n",
    "            \"PGT\": \"Pre-implantation genetic testing type (PGT-A = aneuploidy screening)\",\n",
    "            \"Ref__No_\": \"Reference number for the cycle\",\n",
    "            \"_ET\": \"Number of embryos transferred\",\n",
    "            \"_COC\": \"Number of cumulus-oocyte complexes (oocytes) retrieved\",\n",
    "            \"MII\": \"Number of metaphase II (mature) oocytes\",\n",
    "            \"_2PN\": \"Number of normally fertilized oocytes (2 pronuclei)\",\n",
    "            \"Frozen_oocytes\": \"Number of oocytes frozen (vitrified)\",\n",
    "            \"Frozen_embryo\": \"Number of embryos frozen\",\n",
    "            \"Transferable_Emb_\": \"Number of embryos suitable for transfer\",\n",
    "            \"Age\": \"Patient age at time of cycle\",\n",
    "            \"BMI\": \"Patient BMI at time of cycle\",\n",
    "        },\n",
    "    },\n",
    "    \"reproduction_episodes\": {\n",
    "        \"table_description\": \"Patient reproductive history, demographics, and episode information\",\n",
    "        \"columns\": {\n",
    "            \"Id\": \"Episode identifier\",\n",
    "            \"Date\": \"Episode start date\",\n",
    "            \"Patient_ID\": \"Anonymized patient identifier (SHA-256 hash)\",\n",
    "            \"Gestations\": \"Number of previous pregnancies\",\n",
    "            \"Para\": \"Number of births\",\n",
    "            \"Misscarriage\": \"Number of miscarriages\",\n",
    "            \"E_P\": \"Number of ectopic pregnancies\",\n",
    "            \"C_Sections\": \"Number of cesarean sections\",\n",
    "            \"Preterm_Birth\": \"Number of preterm births\",\n",
    "            \"Partner_ID\": \"Anonymized partner identifier (SHA-256 hash)\",\n",
    "            \"Reason_Consultation\": \"Why the patient consulted (e.g., Primary infertility)\",\n",
    "            \"Therapeutic_Plan\": \"Planned treatment approach (cleaned text)\",\n",
    "            \"Cause_of_the_episode\": \"Clinical cause/diagnosis for this episode (cleaned text)\",\n",
    "            \"Years_of_marriage\": \"Duration of marriage in years\",\n",
    "            \"Nationality\": \"Patient nationality\",\n",
    "            \"Cycle_length\": \"Menstrual cycle length in days\",\n",
    "            \"Previous_treatments\": \"Number of previous fertility treatments\",\n",
    "        },\n",
    "    },\n",
    "    \"spermiogram\": {\n",
    "        \"table_description\": \"Semen analysis results for the male partner\",\n",
    "        \"columns\": {\n",
    "            \"Id\": \"Spermiogram test identifier\",\n",
    "            \"Date\": \"Date the test was performed\",\n",
    "            \"Patient\": \"Male partner identifier (SHA-256 hash)\",\n",
    "            \"Partner\": \"Female partner identifier (SHA-256 hash)\",\n",
    "            \"Volume____1_4_ml_\": \"Semen volume in ml (normal >= 1.4 ml)\",\n",
    "            \"Count____16_M_ml_\": \"Sperm count in millions per ml (normal >= 16 M/ml)\",\n",
    "            \"Progressive____30__\": \"Progressive motility percentage (normal >= 30%)\",\n",
    "            \"Normal_Forms____4__\": \"Normal morphology percentage (normal >= 4%)\",\n",
    "            \"Sperm_Motility_Recovery\": \"Total motile sperm after processing\",\n",
    "            \"Diagnosis\": \"Spermiogram diagnosis (e.g., Normozoospermia, Oligozoospermia)\",\n",
    "            \"Remarks\": \"Additional notes about the sample\",\n",
    "        },\n",
    "    },\n",
    "    \"history_visits\": {\n",
    "        \"table_description\": \"Patient visit history with anthropometric measurements over time\",\n",
    "        \"columns\": {\n",
    "            \"Code\": \"Visit identifier\",\n",
    "            \"Patient_ID\": \"Anonymized patient identifier (SHA-256 hash)\",\n",
    "            \"Date\": \"Visit date\",\n",
    "            \"Age\": \"Patient age at visit\",\n",
    "            \"Height\": \"Patient height in meters\",\n",
    "            \"Weight\": \"Patient weight in kg\",\n",
    "            \"BMI\": \"Body Mass Index (kg/m^2)\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save schema for the LLM to reference\n",
    "with open(\"schema_description.json\", \"w\") as f:\n",
    "    json.dump(SCHEMA_DESCRIPTION, f, indent=2)\n",
    "print(\"Schema descriptions saved to schema_description.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2-create-db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SQLite database\n",
    "conn = create_sqlite_database(data, DB_PATH)\n",
    "\n",
    "# Verify\n",
    "print(\"\\nVerification:\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "for (table,) in cursor.fetchall():\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "    cols = [row[1] for row in cursor.fetchall()]\n",
    "    print(f\"  {table}: {count} rows, {len(cols)} columns\")\n",
    "\n",
    "# Sample queries\n",
    "print(\"\\nSample: SELECT Date, Technique, Outcome FROM cycles LIMIT 3\")\n",
    "for row in cursor.execute(\"SELECT Date, Technique, Outcome FROM cycles LIMIT 3\"):\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "print(\"\\nSample: SELECT Date, Age, BMI FROM history_visits LIMIT 3\")\n",
    "for row in cursor.execute(\"SELECT Date, Age, BMI FROM history_visits ORDER BY Date LIMIT 3\"):\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0-step3-header",
   "metadata": {},
   "source": [
    "## Step 3 — Process Clinical Notes for Vector Search\n",
    "\n",
    "Combines all clinical free-text (follow-up notes, episode text fields, spermiogram remarks)\n",
    "into **one document per patient** after stripping HTML. Then splits into ~300-token chunks\n",
    "with 50-token overlap.\n",
    "\n",
    "Each note is a natural boundary — only notes longer than 300 tokens get sub-chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1-build-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patient_document(data):\n",
    "    \"\"\"\n",
    "    Combine all clinical free-text into a single document.\n",
    "    Sources: follow-up notes, reproduction episode text fields, spermiogram remarks.\n",
    "    All HTML has already been stripped in Step 1.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "\n",
    "    # Follow-up notes (bulk of unstructured text)\n",
    "    notes_df = data[\"follow_up_notes\"].sort_values(\"Date\")\n",
    "    for _, row in notes_df.iterrows():\n",
    "        text = row.get(\"Follow-up_clean\", \"\")\n",
    "        if not text or len(str(text).strip()) < 10:\n",
    "            continue\n",
    "        date = str(row.get(\"Date\", \"\"))[:10]\n",
    "        note_type = str(row.get(\"Type\", \"\"))\n",
    "        sections.append(f\"[Follow-up | {date} | {note_type}] {text}\")\n",
    "\n",
    "    # Reproduction episode free-text fields\n",
    "    repro_df = data[\"reproduction_episodes\"]\n",
    "    for _, row in repro_df.iterrows():\n",
    "        date = str(row.get(\"Date\", \"\"))[:10]\n",
    "        for col in [\"Cause of the episode\", \"Therapeutic Plan\", \"Obstetrics observations\"]:\n",
    "            if col in repro_df.columns:\n",
    "                text = str(row.get(col, \"\"))\n",
    "                if text and len(text.strip()) > 10:\n",
    "                    sections.append(f\"[{col} | {date}] {text}\")\n",
    "\n",
    "    # Spermiogram remarks\n",
    "    for _, row in data[\"spermiogram\"].iterrows():\n",
    "        text = row.get(\"Remarks_clean\", \"\")\n",
    "        if text and len(str(text).strip()) > 10:\n",
    "            date = str(row.get(\"Date\", \"\"))[:10]\n",
    "            sections.append(f\"[Spermiogram Remarks | {date}] {text}\")\n",
    "\n",
    "    return \"\\n\\n\".join(sections)\n",
    "\n",
    "\n",
    "def chunk_document(document, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Split document into overlapping chunks of ~chunk_size tokens.\n",
    "    Uses word count as proxy (1 token ~ 0.75 words).\n",
    "\n",
    "    Each note (separated by double newline) is a natural boundary.\n",
    "    Only notes longer than chunk_size tokens get sub-chunked.\n",
    "    \"\"\"\n",
    "    notes = document.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    max_words = int(chunk_size * 0.75)\n",
    "    overlap_words = int(overlap * 0.75)\n",
    "\n",
    "    for note in notes:\n",
    "        words = note.split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        if len(words) <= max_words:\n",
    "            # Short note — keep as one chunk\n",
    "            chunks.append(note)\n",
    "        else:\n",
    "            # Long note — sub-chunk with sliding window overlap\n",
    "            start = 0\n",
    "            while start < len(words):\n",
    "                end = start + max_words\n",
    "                chunks.append(\" \".join(words[start:end]))\n",
    "                start = end - overlap_words\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2-run-chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the single patient document\n",
    "patient_document = build_patient_document(data)\n",
    "print(f\"Patient document: {len(patient_document):,} characters, {len(patient_document.split()):,} words\")\n",
    "\n",
    "# Chunk it\n",
    "chunks = chunk_document(patient_document)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# Chunk size distribution\n",
    "word_counts = [len(c.split()) for c in chunks]\n",
    "print(f\"Words per chunk: min={min(word_counts)}, max={max(word_counts)}, avg={np.mean(word_counts):.0f}\")\n",
    "print(f\"Approx tokens:  min~{min(word_counts)/0.75:.0f}, max~{max(word_counts)/0.75:.0f}, avg~{np.mean(word_counts)/0.75:.0f}\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\n--- Chunk 0 (first 300 chars) ---\")\n",
    "print(chunks[0][:300])\n",
    "print(f\"\\n--- Chunk 1 (first 300 chars) ---\")\n",
    "print(chunks[1][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0-step4-header",
   "metadata": {},
   "source": [
    "## Step 4 — Embed and Store (Setup)\n",
    "\n",
    "Embeds each text chunk using `BAAI/bge-large-en-v1.5` (1024-dim vectors) and stores\n",
    "them in ChromaDB with the original text and metadata.\n",
    "\n",
    "**Run the cell below when ready.** Requires:\n",
    "```\n",
    "pip install chromadb sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1-embed-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 4: Embed and Store ──\n",
    "# Uncomment the block below and run when dependencies are installed.\n",
    "\n",
    "# import chromadb\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "#\n",
    "# # Load embedding model (1024-dim vectors)\n",
    "# embed_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "#\n",
    "# # Initialize ChromaDB (persistent local storage)\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# collection = chroma_client.get_or_create_collection(\n",
    "#     name=\"clinical_notes\",\n",
    "#     metadata={\"hnsw:space\": \"cosine\"},\n",
    "# )\n",
    "#\n",
    "# # Embed and store all chunks\n",
    "# print(f\"Embedding {len(chunks)} chunks...\")\n",
    "# for i, chunk_text in enumerate(chunks):\n",
    "#     embedding = embed_model.encode(chunk_text, normalize_embeddings=True).tolist()\n",
    "#     collection.add(\n",
    "#         ids=[f\"chunk_{i}\"],\n",
    "#         embeddings=[embedding],\n",
    "#         documents=[chunk_text],\n",
    "#         metadatas=[{\"chunk_index\": i}],\n",
    "#     )\n",
    "#     if (i + 1) % 50 == 0:\n",
    "#         print(f\"  Embedded {i + 1}/{len(chunks)} chunks...\")\n",
    "#\n",
    "# print(f\"\\nStored {collection.count()} chunks in ChromaDB (./chroma_db)\")\n",
    "# print(\"Vector index ready for semantic search.\")\n",
    "\n",
    "print(\"Step 4 code is ready. Uncomment and run when chromadb + sentence-transformers are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "After running Steps 1-3, you have:\n",
    "\n",
    "| Data Store | Contents | File |\n",
    "|------------|----------|------|\n",
    "| **SQLite** | 4 structured tables (cycles, reproduction_episodes, spermiogram, history_visits) | `clinic.db` |\n",
    "| **Text chunks** | Clinical notes ready for embedding | `chunks` list in memory |\n",
    "| **Schema** | Column descriptions for LLM text-to-SQL | `schema_description.json` |\n",
    "| **ID mapping** | SHA-256 hash <-> original identifier | `anonymization_mapping.json` |\n",
    "\n",
    "After running Step 4, you also have:\n",
    "\n",
    "| Data Store | Contents | File |\n",
    "|------------|----------|------|\n",
    "| **ChromaDB** | Embedded note chunks (1024-dim vectors) | `./chroma_db/` |\n",
    "\n",
    "These two data stores (SQLite + ChromaDB) are used side-by-side in Phase 2 (Query Pipeline)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}